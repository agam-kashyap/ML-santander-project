{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport gc\nimport random\nimport time\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler    \nimport lightgbm as lgb\nfrom collections import defaultdict\nimport joblib\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nscaler = StandardScaler()\nminmax = MinMaxScaler()\n\ntrain_file = \"/kaggle/input/santander-pr/train.csv\"\ntest_file = \"/kaggle/input/santander-pr/test.csv\"\n\ntargetcols = [\"ind_ahor_fin_ult1\",\"ind_aval_fin_ult1\",\"ind_cco_fin_ult1\",\"ind_cder_fin_ult1\",\"ind_cno_fin_ult1\",\"ind_ctju_fin_ult1\",\"ind_ctma_fin_ult1\",\n              \"ind_ctop_fin_ult1\",\"ind_ctpp_fin_ult1\",\"ind_deco_fin_ult1\",\"ind_deme_fin_ult1\",\"ind_dela_fin_ult1\", \"ind_ecue_fin_ult1\",\"ind_fond_fin_ult1\",\n              \"ind_hip_fin_ult1\", \"ind_plan_fin_ult1\",\"ind_pres_fin_ult1\",\"ind_reca_fin_ult1\",\"ind_tjcr_fin_ult1\",\"ind_valo_fin_ult1\",\"ind_viv_fin_ult1\",\n              \"ind_nomina_ult1\",\"ind_nom_pens_ult1\",\"ind_recibo_ult1\"]\n\ndtype_list = {'ind_cco_fin_ult1': 'uint8',\n              'ind_deme_fin_ult1': 'uint8',\n              'ind_aval_fin_ult1': 'uint8',\n              'ind_valo_fin_ult1': 'uint8',\n              'ind_reca_fin_ult1': 'uint8',\n              'ind_ctju_fin_ult1': 'uint8',\n              'ind_cder_fin_ult1': 'uint8', \n              'ind_plan_fin_ult1': 'uint8',\n              'ind_fond_fin_ult1': 'uint8', \n              'ind_hip_fin_ult1': 'uint8',\n              'ind_pres_fin_ult1': 'uint8', \n              'ind_nomina_ult1': 'float64', \n              'ind_cno_fin_ult1': 'uint8',\n              'ind_ctpp_fin_ult1': 'uint8',\n              'ind_ahor_fin_ult1': 'uint8',\n              'ind_dela_fin_ult1': 'uint8',\n              'ind_ecue_fin_ult1': 'uint8',\n              'ind_nom_pens_ult1': 'float64',\n              'ind_recibo_ult1': 'uint8',\n              'ind_deco_fin_ult1': 'uint8',\n              'ind_tjcr_fin_ult1': 'uint8', \n              'ind_ctop_fin_ult1': 'uint8',\n              'ind_viv_fin_ult1': 'uint8',\n              'ind_ctma_fin_ult1': 'uint8',\n             'ncodpers' : 'uint32'}  \n\nfeature_cols = ['ncodpers','fecha_dato','age','renta','nomprov', 'ind_nuevo', \n               'segmento', 'ind_actividad_cliente', 'pais_residencia', 'ind_empleado', \n                'sexo', 'tiprel_1mes', 'indrel_1mes', 'antiguedad',  'indrel', 'indext', 'indresi', 'indfall', 'canal_entrada']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modification Functions\n### Helper Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def string_num_age(x):\n    if(type(x) == str and x != ' NA'):\n        x = int(x)\n    elif( x == ' NA'):\n        x = np.nan\n    return x\n\ndef string_num_senior(x):\n    if(type(x) == str and x != '     NA'):\n        x = int(x)\n    elif( x == '     NA'):\n        x = np.nan\n    return x\n\ndef string_num_primary(x):\n    if(type(x) == str and x!= np.nan and x!='P'):\n        x = float(x)\n    elif(type(x) == float and math.isnan(x)==False):\n        x = int(x)\n    elif(x == 'P'):\n        x = 2.5\n    return x\n\ndef modify_age(train, test):\n    print(\"Modifying...age\")\n    temp_train = train.copy()\n    temp_test = test.copy()\n    temp_train.age = temp_train.age.apply(lambda x: string_num_age(x))\n    temp_train = temp_train.loc[temp_train.age.isnull()==False]\n    temp_train.age = np.where(temp_train.age < 14, 14, temp_train.age)\n    temp_train.age = np.where(temp_train.age > 90, 90, temp_train.age)\n    temp_test.age = np.where(temp_test.age < 14, 14, temp_test.age)\n    temp_test.age = np.where(temp_test.age > 90, 90, temp_test.age)\n\n    temp_train.age = minmax.fit_transform(np.array(temp_train.age).reshape(-1,1))\n    temp_test.age = minmax.fit_transform(np.array(temp_test.age).reshape(-1,1))\n    \n    return temp_train, temp_test\n\ndef modify_renta(train, test):\n    temp_train = train.copy()\n    temp_test = test.copy()\n\n    temp_train.age = temp_train.age.apply(lambda x: string_num_age(x))\n    temp_train = temp_train.loc[temp_train.age.isnull()==False]\n    \n    temp_train.nomprov = temp_train.nomprov.fillna(temp_train.nomprov.mode()[0])\n    temp_train.nomprov = temp_train.nomprov.apply(lambda x: nom_mod(x))\n\n    print('Modifying train...renta')\n    province = temp_train.nomprov.unique()\n    median = np.zeros((len(province),1))\n    for i in range(len(province)):\n        median[i] = temp_train[(temp_train[\"nomprov\"]==province[i])]['renta'].median()\n    print('Train Medians found ->')\n    print(median)\n\n    for i in range(len(province)):\n        temp_train.renta = np.where((temp_train.nomprov == province[i]) & (temp_train.renta.isnull()==True), median[i], temp_train.renta)\n\n    del median\n    \n    temp_test.age = temp_test.age.apply(lambda x: string_num_age(x))\n    temp_test = temp_test.loc[temp_test.age.isnull()==False]\n    \n    temp_test.nomprov = temp_test.nomprov.fillna(temp_test.nomprov.mode()[0])\n    temp_test.nomprov = temp_test.nomprov.apply(lambda x: nom_mod(x))\n    \n    print('Modifying test...renta')\n    province = temp_test.nomprov.unique()\n    median = np.zeros((len(province),1))\n    for i in range(len(province)):\n        median[i] = temp_test[(temp_test[\"nomprov\"]==province[i])]['renta'].median()\n    print('Test Medians found ->')\n    print(median)\n\n    for i in range(len(province)):\n        temp_test.renta = np.where((temp_test.nomprov == province[i]) & (temp_test.renta.isnull()==True), median[i], temp_test.renta)\n\n    del median\n    \n    temp_train.renta = scaler.fit_transform(np.array(temp_train.loc[:,'renta']).reshape(-1,1))\n    temp_test.renta = scaler.fit_transform(np.array(temp_test.loc[:,'renta']).reshape(-1,1))\n    return temp_train, temp_test\n\ndef modify_segmento(train, test):\n    print(\"Modifying....segmento\")\n    temp_train = train.copy()\n    temp_test = test.copy()\n    temp_train.segmento = temp_train.segmento.fillna(temp_train.segmento.mode()[0])\n    temp_test.segmento = temp_test.segmento.fillna(temp_test.segmento.mode()[0])\n    return temp_train, temp_test\n\ndef modify_sexo(train, test):\n    print(\"Modifying....sexo\")\n    temp_train = train.copy()\n    temp_test = test.copy()\n    temp_train.sexo = temp_train.sexo.fillna(value=temp_train.sexo.mode()[0])\n    temp_test.sexo = temp_test.sexo.fillna(value=temp_test.sexo.mode()[0])\n    return temp_train, temp_test\n\ndef modify_antiguedad(train, test):\n    print(\"Modifying....antiguedad\")\n    temp_train = train.copy()\n    temp_test = test.copy()\n    temp_train.antiguedad = temp_train.antiguedad.apply(lambda x: string_num_senior(x))\n    temp_test.antiguedad = temp_test.antiguedad.apply(lambda x: string_num_senior(x))\n    \n    temp_train.antiguedad = temp_train.antiguedad.fillna(value=-999999)\n    temp_test.antiguedad = temp_test.antiguedad.fillna(value=-999999)\n    temp_train.antiguedad = np.where(temp_train.antiguedad==-999999,-1,temp_train.antiguedad)\n    temp_test.antiguedad = np.where(temp_test.antiguedad==-999999,-1,temp_test.antiguedad)\n    temp_train.antiguedad = minmax.fit_transform(np.array(temp_train.loc[:,'antiguedad']).reshape(-1,1))\n    temp_test.antiguedad = minmax.fit_transform(np.array(temp_test.loc[:,'antiguedad']).reshape(-1,1))\n    temp_train.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='signed')\n    temp_train.select_dtypes(include=['float']).apply(pd.to_numeric,downcast='signed')\n    temp_test.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='signed')\n    temp_test.select_dtypes(include=['float']).apply(pd.to_numeric,downcast='signed')\n    return temp_train, temp_test\n\ndef modify_fecha_dato(train, test):\n    print(\"Modifying....fecha_dato\")\n    temp_train = train.copy()\n    temp_test = test.copy()\n    temp_train.age = temp_train.age.apply(lambda x: string_num_age(x))\n    temp_test.age = temp_test.age.apply(lambda x: string_num_age(x))\n    temp_test = temp_test.loc[temp_test.age.isnull()==False]\n    temp_train = temp_train.loc[temp_train.age.isnull()==False]\n    temp_train.fecha_dato = temp_train['fecha_dato'].apply(lambda x: 100*x.year + x.month)\n    temp_test.fecha_dato = temp_test['fecha_dato'].apply(lambda x: 100*x.year + x.month)\n    return temp_train, temp_test\n\ndef modify_fecha_alta(train, test):\n    print(\"Modifying....fecha_alta\")\n    temp_train = train.copy()\n    temp_test = test.copy()\n    temp_train.age = temp_train.age.apply(lambda x: string_num_age(x))\n    temp_test.age = temp_test.age.apply(lambda x: string_num_age(x))\n    temp_test = temp_test.loc[temp_test.age.isnull()==False]\n    temp_train = temp_train.loc[temp_train.age.isnull()==False]\n    temp_train.fecha_alta = temp_train['fecha_alta'].apply(lambda x: 100*x.year + x.month)\n    temp_test.fecha_alta = temp_test['fecha_alta'].apply(lambda x: 100*x.year + x.month)\n    return temp_train, temp_test\n\ndef modify_indrel_1mes(train, test):\n    print(\"Modifying...indrel_1mes\")\n    temp_train = train.copy()\n    temp_test = test.copy()\n    temp_train.indrel_1mes = temp_train.indrel_1mes.apply(lambda x: string_num_primary(x))\n    temp_test.indrel_1mes = temp_test.indrel_1mes.apply(lambda x: string_num_primary(x))\n    temp_train.indrel_1mes = temp_train.indrel_1mes.fillna(temp_train.indrel_1mes.median())\n    temp_test.indrel_1mes = temp_test.indrel_1mes.fillna(temp_test.indrel_1mes.median())\n    return temp_train, temp_test\n\ndef pais_mod(x):\n    pais = ['ES','FR','AR','DE','GB','US','CO','IT','RO','MX']\n    if( x not in pais):\n        x = 'Outside'\n    return x\n    \ndef modify_pais_residencia(train, test):\n    temp_train = train.copy()\n    temp_test = test.copy()\n    temp_train.pais_residencia = temp_train.pais_residencia.apply(lambda x: pais_mod(x))\n    temp_test.pais_residencia = temp_test.pais_residencia.apply(lambda x: pais_mod(x))\n    return temp_train, temp_test\n\ndef canal_mod(x):\n    canal = ['KHE','KAT','KFC','KHQ','KFA','KHK','KHM','KHD','KHN','KAS']\n    if( x not in canal):\n        x = 'UNK'\n    return x\n    \ndef modify_canal_entrada(train, test):\n    temp_train = train.copy()\n    temp_test = test.copy()\n    temp_train.canal_entrada = temp_train.canal_entrada.fillna(temp_train.canal_entrada.mode()[0])\n    temp_test.canal_entrada = temp_test.canal_entrada.fillna(temp_test.canal_entrada.mode()[0])\n    temp_train.canal_entrada = temp_train.canal_entrada.apply(lambda x: canal_mod(x))\n    temp_test.canal_entrada = temp_test.canal_entrada.apply(lambda x: canal_mod(x))\n    return temp_train, temp_test\n\ndef nom_mod(x):\n    nomprov = ['MADRID','BARCELONA','VALENCIA','SEVILLA','CORUÑA, A','MURCIA','MALAGA','ZARAGOZA','ALICANTE','CADIZ']\n    if x not in nomprov:\n        x = 'OTHER'\n    return x\n\nnom_dict = {\n    'MADRID': 'M',\n    'BARCELONA' : 'B',\n    'VALENCIA' : 'V',\n    'SEVILLA' : 'S',\n    'CORUÑA, A' : 'C',\n    'MURCIA' : 'M1',\n    'MALAGA': 'M2',\n    'ZARAGOZA' : 'Z',\n    'ALICANTE' : 'A1',\n    'CADIZ' : 'C1',\n    'OTHER' : 'O'\n}\ndef modify_nomprov(train, test):\n    temp_train = train.copy()\n    temp_test = test.copy()\n    temp_train.nomprov = temp_train.nomprov.fillna(temp_train.nomprov.mode()[0])\n    temp_test.nomprov = temp_test.nomprov.fillna(temp_test.nomprov.mode()[0])\n    temp_train.nomprov = temp_train.nomprov.apply(lambda x: nom_mod(x))\n    temp_test.nomprov = temp_test.nomprov.apply(lambda x: nom_mod(x))\n    temp_test.nomprov = temp_test.nomprov.apply(lambda x: nom_dict[x])\n    temp_train.nomprov = temp_train.nomprov.apply(lambda x: nom_dict[x])\n    return temp_train, temp_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Memory management Code"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(props, columns_now):\n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    for col in columns_now:\n        print(col)\n        if props[col].dtype != object:  # Exclude strings\n\n            print(\"******************************\")\n            print(\"dtype before: \",props[col].dtype)\n\n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n\n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n\n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n\n            else:\n                props[col] = props[col].astype(np.float32)\n\n        print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n        mem_usg = props.memory_usage().sum() / 1024**2 \n        print(\"Memory usage is: \",mem_usg,\" MB\")\n        print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataframe Creation"},{"metadata":{},"cell_type":"markdown","source":"### Reading CSV"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"x_train = pd.read_csv(train_file, usecols=feature_cols, parse_dates=['fecha_dato'])\nx_test = pd.read_csv(test_file, usecols=feature_cols, parse_dates=['fecha_dato'])\nx_train.fecha_dato = x_train['fecha_dato'].apply(lambda x: 100*x.year + x.month)\nx_test.fecha_dato = x_test['fecha_dato'].apply(lambda x: 100*x.year + x.month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = []\nx_train, x_test = modify_age(x_train, x_test)\ncol_to_drop = []\nfor idx,col in enumerate(x_train.columns):\n    \n    print(\"Reading....\" + str(col))\n\n    if col == 'age' or col == 'fecha_dato':\n        continue\n\n    elif col == \"renta\":\n        x_train, x_test = modify_renta(x_train, x_test)\n        print(col + \"...Done!\")\n\n    elif col == \"segmento\":\n        x_train, x_test = modify_segmento(x_train, x_test)\n        print(col + \"...Done!\")\n\n    elif col == 'sexo':\n        x_train, x_test = modify_sexo(x_train, x_test)\n        print(col + \"...Done!\")\n\n    elif col == \"ind_nuevo\":\n        print(\"Modifying....\"+col)\n        x_train.ind_nuevo = x_train.ind_nuevo.fillna(value=1)\n        x_test.ind_nuevo = x_test.ind_nuevo.fillna(value=1)\n        print(col + \"...Done!\")\n\n    elif col == \"antiguedad\":\n        x_train, x_test = modify_antiguedad(x_train, x_test)\n        print(col + \"...Done!\")\n\n    elif col == 'indrel':\n        print(\"Modifying....\"+col)\n        x_train.indrel = x_train.indrel.fillna(value=1)\n        x_test.indrel = x_test.indrel.fillna(value=1)\n        print(col + \"...Done!\")\n\n    elif col == 'tiprel_1mes':\n        print(\"Modifying....\"+col)\n        x_train.tiprel_1mes = x_train.tiprel_1mes.fillna(x_train.tiprel_1mes.mode()[0])\n        x_train.tiprel_1mes = np.where((x_train.tiprel_1mes=='N')|(x_train.tiprel_1mes=='R'), 'I',x_train.tiprel_1mes)\n        x_test.tiprel_1mes = x_test.tiprel_1mes.fillna(x_test.tiprel_1mes.mode()[0])\n        x_test.tiprel_1mes = np.where((x_test.tiprel_1mes=='N')|(x_test.tiprel_1mes=='R'), 'I',x_test.tiprel_1mes)\n        print(col + \"...Done!\")\n\n    elif col == 'indext':\n        print(\"Modifying....\"+col)\n        x_train.indext = x_train.indext.fillna(value='U')\n        x_test.indext = x_test.indext.fillna(value='U')\n        print(col + \"...Done!\")\n\n    elif col == \"ind_actividad_cliente\":\n        print(\"modifying...\"+col)\n        print(col + \"...Done!\")\n\n    elif col== 'ncodpers':\n        print(\"Modifying....\"+col)\n        ids = x_test.ncodpers.unique()\n        print(col + \"...Done!\")\n\n    elif col == \"nomprov\":\n        print(\"Modifying....\"+ col)\n        x_train, x_test = modify_nomprov(x_train, x_test)\n        print(col + \"...Done!\")\n\n    elif col == 'fecha_alta':\n        x_train, x_test = modify_fecha_alta(x_train, x_test)\n        print(col + \"...Done!\")\n\n    elif col == 'pais_residencia':\n        print(\"Modifying....\"+ col)\n        x_train, x_test = modify_pais_residencia(x_train, x_test)\n        print(col + \"...Done!\")\n\n    elif col == 'canal_entrada':\n        print(\"Modifying....\"+col)\n        x_train, x_test = modify_canal_entrada(x_train, x_test)\n        print(col + \"...Done!\")\n\n    elif col == 'indrel_1mes':\n        x_train, x_test = modify_indrel_1mes(x_train, x_test)\n        print(col + \"...Done!\")\n\n    else: \n        print(\"Modifying....\"+ col)\n        print(col + \"...Done!\")\n\n    '''''''''Null values filled'''''''''''\n    columns_now = []\n\n    if x_train[col].dtype == 'object':\n        x_train[col] = x_train[col].fillna(x_train[col].mode()[0])\n        cat_enc_train = pd.get_dummies(x_train[col], prefix=col)\n        cat_enc_test = pd.get_dummies(x_test[col], prefix=col)\n        for i in cat_enc_train.columns.to_list():\n            columns_now.append(i)\n        x_train = pd.concat([x_train, cat_enc_train], axis=1)\n        x_test = pd.concat([x_test, cat_enc_test], axis=1)\n        col_to_drop.append(col)\n    \n    else:\n        if(col != 'fecha_dato' and col!= 'fecha_alta' and col!='ncodpers'):\n            columns_now.append(col)\n        continue\n        \n    del cat_enc_train, cat_enc_test\n    x_train.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='unsigned')\n    x_train.select_dtypes(include=['float']).apply(pd.to_numeric,downcast='unsigned')\n    x_test.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='unsigned')\n    x_test.select_dtypes(include=['float']).apply(pd.to_numeric,downcast='unsigned')\n    x_train = reduce_mem_usage(x_train, columns_now)\n    print(\"Train Mem reduction...Done!\")\n    x_test = reduce_mem_usage(x_test, columns_now)\n    print(\"Test Mem reduction...Done!\")\n\nfor i in col_to_drop:\n    x_train.drop(columns=[i], inplace=True)\n    x_test.drop(columns=[i], inplace=True)\n    \nprint(x_train.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = pd.read_csv(train_file, usecols = ['ncodpers','age','fecha_dato','ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1'], dtype=dtype_list, parse_dates=['fecha_dato'])\ny_train.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='unsigned')\ny_train.select_dtypes(include=['float']).apply(pd.to_numeric,downcast='unsigned')\n\n# Selection of rows\ny_train.age = y_train.age.apply(lambda x: string_num_age(x))\ny_train = y_train.loc[y_train.age.isnull()==False]\ny_train.fecha_dato = y_train['fecha_dato'].apply(lambda x: 100*x.year + x.month)\ny_train = y_train.fillna(0)\ny_train.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='unsigned')\ny_train.select_dtypes(include=['float']).apply(pd.to_numeric,downcast='unsigned')\n\ny_train = reduce_mem_usage(y_train,y_train.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lags"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_lags(lag, date, x_train, df_name):\n    \n    for i in lag:\n        if(i==0):\n            break\n        rename_dict = {}\n        col_names = []\n        for j in targetcols:\n            name = j + '_lag_' + str(i)\n            rename_dict[j] = name\n            col_names.append(name)\n        df = pd.DataFrame()\n        for j in date:\n            cur = j-i\n            if(cur <= 201500):\n                dum = x_train[x_train.fecha_dato == j]\n                df_lag = y_train[y_train.fecha_dato==j]\n                df_lag = df_lag.rename(columns=rename_dict)\n                df_lag.drop(columns=['fecha_dato','age'],inplace=True)\n                for k in col_names:\n                    df_lag[k] = 0\n                dum = dum.merge(df_lag, on=['ncodpers'], how='left')\n                df = pd.concat([df,dum], axis=0)\n                del dum\n            else:\n                if((j > 201600) and cur not in range(201501, 201512) and cur not in range(201601, 201605)):\n                    cur = 201512 - (201600-cur)\n                df_lag = y_train[y_train.fecha_dato==cur]\n                df_lag = df_lag.rename(columns=rename_dict)\n                df_lag.drop(columns=['fecha_dato','age'],inplace=True)\n                dum = x_train[x_train.fecha_dato == j]\n                dum = dum.merge(df_lag, on=['ncodpers'], how='left')\n                df = pd.concat([df,dum], axis=0)\n                print(\"1_>\"+str(j)+\"->\"+str(dum.shape))\n                del dum\n        x_train = df\n        del df\n        print(\"1--->\"+str(x_train.shape))\n        print('Lag '+str(i)+' for ' + df_name +'...Done!!')\n    x_train.fillna(0, inplace=True)\n    return x_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Two way split timeframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"lags_1 = [1,2,3,4]\ndate_1 = [201503,201504,201505,201506,201507]\nlags_2 = [1,2,3,4,5]\ndate_2 = [201604]\n\nx_train_1 = create_lags(lags_1, date_1, x_train, 'x_train_1')\nx_train_1= reduce_mem_usage(x_train_1, x_train_1.columns)\nx_test_1 = create_lags(lags_1, [201605], x_test, 'x_test_1')\nx_test_1=reduce_mem_usage(x_test_1, x_test_1.columns)\ny_train_1 = y_train[((y_train.fecha_dato>=201503) & (y_train.fecha_dato<=201507))]\ny_train_1=reduce_mem_usage(y_train_1, y_train_1.columns) \n\nx_train_2 = create_lags(lags_2, date_2, x_train, 'x_train_2')\nx_train_2=reduce_mem_usage(x_train_2, x_train_2.columns)\nx_test_2 = create_lags(lags_2, [201605], x_test, 'x_test_2')\nx_test_2=reduce_mem_usage(x_test_2, x_test_2.columns)\ny_train_2 = y_train[((y_train.fecha_dato==201604))]\ny_train_2=reduce_mem_usage(y_train_2, y_train_2.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recent_prod = y_train[y_train.fecha_dato==201604]\n\ndel y_train\ndel x_train, x_test\n\nrecent_prod.drop(columns=['fecha_dato'], inplace=True)\nrecent_prod = reduce_mem_usage(recent_prod, recent_prod.columns)\n\nproduct_col = recent_prod.columns.tolist()\nfor i in ['ncodpers','age']:\n    product_col.remove(i)\n    \nids = x_test_1['ncodpers'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model \n### Weighted Average LGBM "},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nimport joblib\n\nid_preds = defaultdict(list)\nids = x_test_1['ncodpers'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First models for feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'boosting_type': 'gbdt',\n          'max_depth' : -1,\n          'objective': 'binary',\n          'num_leaves': 64,\n          'learning_rate': 0.1,\n          'num_iterations': 200,\n          'max_bin': 512,\n          'subsample_for_bin': 200,\n          'subsample': 1,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'reg_alpha': 5,\n          'reg_lambda': 10,\n          'min_split_gain': 0.5,\n          'min_child_weight': 1,\n          'min_child_samples': 5,\n          'scale_pos_weight': 1,\n          'num_class' : 1,\n          'metric' : 'binary_error',\n         'verbosity' : 1}\n\nid_preds = defaultdict(list)\ncombined_2015 = {}\n\nfor c in product_col:\n    print(c)\n    print(c+\"-first\")\n    y_t_1 = y_train_1[c]\n    x_t_1 = x_train_1.drop(['fecha_dato','ncodpers'],1)\n    model_1 = lgb.LGBMClassifier(\n        boosting_type= 'gbdt',\n        objective = 'binary',\n        max_depth = params['max_depth'],\n        max_bin = params['max_bin'],\n        subsample = params['subsample'],\n        subsample_freq = params['subsample_freq'],\n        min_split_gain = params['min_split_gain'],\n        min_child_weight = params['min_child_weight'],\n        min_child_samples = params['min_child_samples'],\n        scale_pos_weight = params['scale_pos_weight'],\n        learning_rate=params['learning_rate'],\n        num_iterations=params['num_iterations'],\n        verbosity = params['verbosity']\n    )\n    \n    model_1.fit(x_t_1,y_t_1)\n    x_t2_1 = x_test_1.drop(['fecha_dato','ncodpers'],1)\n    prediction_1 = model_1.predict_proba(x_t2_1)[:,1]\n    combined_2015[c] = model_1\n    del x_t_1, y_t_1, x_t2_1, model_1\n\njoblib.dump(combined_2015,'./combined2015.pkl')\n\ncombined_2016 = {}\nfor c in product_col:\n    print(c+\"-second\")\n    y_t_2 = y_train_2[c]\n    x_t_2 = x_train_2.drop(['fecha_dato','ncodpers'],1)\n    model_2 = lgb.LGBMClassifier(\n        boosting_type= 'gbdt',\n        objective = 'binary',\n        max_depth = params['max_depth'],\n        max_bin = params['max_bin'],\n        subsample = params['subsample'],\n        subsample_freq = params['subsample_freq'],\n        min_split_gain = params['min_split_gain'],\n        min_child_weight = params['min_child_weight'],\n        min_child_samples = params['min_child_samples'],\n        scale_pos_weight = params['scale_pos_weight'],\n        learning_rate=params['learning_rate'],\n        num_iterations=params['num_iterations'],\n        verbosity = params['verbosity']\n    )\n    \n    model_2.fit(x_t_2,y_t_2)\n    x_t2_2 = x_test_2.drop(['fecha_dato','ncodpers'],1)\n    prediction_2 = model_2.predict_proba(x_t2_2)[:,1]\n    combined_2016[c] = model_2\n    del x_t_2, y_t_2, x_t2_2, model_2\n    \n\njoblib.dump(combined_2016,'./combined2016.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_15 = combined_2015\nmodel_16 = combined_2016","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection on Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = [0 for i in range(0,155)]\nfor i in product_col:\n    a+= model_15[i].feature_importances_\nprint(len(a))\nprint(len(x_train_1.drop(columns=['fecha_dato','ncodpers']).columns))\nfeat_imp_1 = {}\nfor i in zip(x_train_1.drop(columns=['fecha_dato','ncodpers']).columns,a):\n    feat_imp_1[i[0]] = i[1]\n\nfeat_imp_1 = sorted(feat_imp_1.items(), key = lambda x: x[1], reverse=False)\nfor i in feat_imp_1:\n    print(i)\n\na = [0 for i in range(len(x_train_2.drop(columns=['fecha_dato','ncodpers']).columns))]\nfor i in product_col:\n    a+= model_16[i].feature_importances_\nprint(len(a))\nprint(len(x_train_2.drop(columns=['fecha_dato','ncodpers']).columns))\nfeat_imp_2 = {}\nfor i in zip(x_train_2.drop(columns=['fecha_dato','ncodpers']).columns,a):\n    feat_imp_2[i[0]] = i[1]\n\nfeat_imp_2 = sorted(feat_imp_2.items(), key = lambda x: x[1], reverse=False)\nfor i in feat_imp_2:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_2015, drop_2016 = [],[]\nfor i in feat_imp_1:\n    if(i[1]<5):\n        drop_2015.append(i[0])\n\nfor i in feat_imp_2:\n    if(i[1]<5):\n        drop_2016.append(i[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Second model post feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom collections import defaultdict\n\nparams = {'boosting_type': 'gbdt',\n          'max_depth' : -1,\n          'objective': 'binary',\n          'num_leaves': 64,\n          'learning_rate': 0.1,\n          'num_iterations': 200,\n          'max_bin': 512,\n          'subsample_for_bin': 200,\n          'subsample': 1,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'reg_alpha': 5,\n          'reg_lambda': 10,\n          'min_split_gain': 0.5,\n          'min_child_weight': 1,\n          'min_child_samples': 5,\n          'scale_pos_weight': 1,\n          'num_class' : 1,\n          'metric' : 'binary_error',\n         'verbosity' : 1}\n\nid_preds = defaultdict(list)\nids = x_test_1['ncodpers'].values\n\nnew_x_train_1 = x_train_1.drop(columns=drop_2015)\nnew_x_test_1 = x_test_1.drop(columns=drop_2015)\ndel x_train_1, x_test_1\n\npredictions_2015 = {}\nmodels_2015 = {}\nfor c in product_col:\n    print(c+\"-first\")\n    y_t_1 = y_train_1[c]\n    x_t_1 = new_x_train_1.drop(['fecha_dato','ncodpers'],1)\n    model_1 = lgb.LGBMClassifier(\n        boosting_type= 'gbdt',\n        objective = 'binary',\n        max_depth = params['max_depth'],\n        max_bin = params['max_bin'],\n        subsample = params['subsample'],\n        subsample_freq = params['subsample_freq'],\n        min_split_gain = params['min_split_gain'],\n        min_child_weight = params['min_child_weight'],\n        min_child_samples = params['min_child_samples'],\n        scale_pos_weight = params['scale_pos_weight'],\n        learning_rate=params['learning_rate'],\n        num_iterations=params['num_iterations'],\n        verbosity = params['verbosity']\n    )\n    \n    model_1.fit(x_t_1,y_t_1)\n    x_t2_1 = new_x_test_1.drop(['fecha_dato','ncodpers'],1)\n    prediction_1 = model_1.predict_proba(x_t2_1)[:,1]\n    models_2015[c] = model_1\n    del x_t_1, y_t_1, x_t2_1, model_1\n    predictions_2015[c] = prediction_1\n    \n\njoblib.dump(models_2015,'./Newcombined2015.pkl')\n\nnew_x_train_2 = x_train_2.drop(columns=drop_2016)\nnew_x_test_2 = x_test_2.drop(columns=drop_2016)\ndel x_train_2, x_test_2\n\npredictions_2016 = {}\nmodels_2016 = {}\nfor c in product_col:\n    print(c+\"-second\")\n    y_t_2 = y_train_2[c]\n    x_t_2 = new_x_train_2.drop(['fecha_dato','ncodpers'],1)\n    model_2 = lgb.LGBMClassifier(\n        boosting_type= 'gbdt',\n        objective = 'binary',\n        max_depth = params['max_depth'],\n        max_bin = params['max_bin'],\n        subsample = params['subsample'],\n        subsample_freq = params['subsample_freq'],\n        min_split_gain = params['min_split_gain'],\n        min_child_weight = params['min_child_weight'],\n        min_child_samples = params['min_child_samples'],\n        scale_pos_weight = params['scale_pos_weight'],\n        learning_rate=params['learning_rate'],\n        num_iterations=params['num_iterations'],\n        verbosity = params['verbosity']\n    )\n    \n    model_2.fit(x_t_2,y_t_2)\n    x_t2_2 = new_x_test_2.drop(['fecha_dato','ncodpers'],1)\n    prediction_2 = model_2.predict_proba(x_t2_2)[:,1]\n    models_2016[c] = model_2\n    del x_t_2, y_t_2, x_t2_2, model_2\n    predictions_2016[c] = prediction_2\n    \n\njoblib.dump(models_2016,'./Newcombined2016.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Weighted Average"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nid_preds = defaultdict(list)\n\nfor c in product_col:\n    print(c)\n    prediction = predictions_2015[c]*0.2 + predictions_2016[c]*0.8\n    for id, p in zip(ids, prediction):\n        id_preds[id].append(p)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Prediction Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ntrain_preds = {}\n\nfor id, p in tqdm(id_preds.items(), desc='Loading....'):\n    try:\n        recent = recent_prod[recent_prod.ncodpers==id].iloc[0]\n        preds = {}\n        for i in zip(tuple(product_col),p):\n            if(recent[i[0]] == 1):\n                preds[i[0]] = 1 - i[1]\n            else:\n                preds[i[0]] = i[1]\n\n        temp_fin = sorted(preds.items(), key = lambda x: x[1], reverse=True)[:5]  #Dict\n        preds_fin = []\n        for i in temp_fin:\n            preds_fin.append(i[0])\n        train_preds[id] = preds_fin\n    except:\n        for i in zip(tuple(product_col),p):\n            preds[i[0]] = i[1]\n        temp_fin = sorted(preds.items(), key = lambda x: x[1], reverse=True)[:5]\n        preds_fin = []\n        for i in temp_fin:\n            preds_fin.append(i[0])\n        train_preds[id] = preds_fin\n\ndf = {\n    'ncodpers': [],\n    'changed' : []\n}\nfor i in train_preds:\n    df['ncodpers'].append(i)\n    prods = ''\n    for j in train_preds[i]:\n        prods += \" \" + j\n    df['changed'].append(prods)\n\nprint(df)\nfinal_df = pd.DataFrame(df, columns = ['ncodpers','changed'])\nfinal_df.to_csv('/kaggle/working/lgbm_sub1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}